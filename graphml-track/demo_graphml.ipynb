{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator, GraphSAGENodeGenerator, HinSAGENodeGenerator\n",
    "from stellargraph.layer import GCN, GAT, GraphSAGE, HinSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import vertices and edges data\n",
    "v_data = pd.read_csv(\"data/polimi.case.graphs.vertices.csv\", sep=',', index_col='node_id', low_memory=False)\n",
    "e_data = pd.read_csv(\"data/polimi.case.graphs.edges.csv\", sep=',', index_col='edge_id', low_memory=False)\n",
    "#import core and extended targets for training\n",
    "core_training = pd.read_csv(\"data/training.core.vertices.csv\", sep='\\t', index_col='NodeID')      \n",
    "ext_training = pd.read_csv(\"data/training.extended.vertices.csv\", sep='\\t', index_col='NodeID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsample edges\n",
    "e_sample = e_data.sample(n=3000)\n",
    "#gather vertices from edges\n",
    "v_list = list(pd.Categorical(list(e_sample.to_id)+list(e_sample.from_id)).categories)\n",
    "#subsample vertices\n",
    "v_sample = v_data[v_data.index.isin(v_list)]\n",
    "#subsample targets\n",
    "core_training_sample = core_training[core_training.index.isin(v_list)] \n",
    "ext_training_sample = ext_training[ext_training.index.isin(v_list)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set missing Core/Extended ID to 0 -> could be a way, but other solutions are appreciated\n",
    "v_sample.CoreCaseGraphID = v_sample.CoreCaseGraphID.fillna(0)\n",
    "v_sample.ExtendedCaseGraphID = v_sample.ExtendedCaseGraphID.fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes for each kynd of node (['Account', 'Address', 'Customer', 'Derived Entity', 'External Entity'])\n",
    "#In a dumb but explicable way\n",
    "# Account: 'Revenue Size Flag', 'Account ID String', 'CoreCaseGraphID', 'ExtendedCaseGraphID', 'testingFlag'\n",
    "v_account = v_sample[v_sample.Label == 'Account']\n",
    "v_account = v_account.drop(['Label', 'Address', 'Person or Organisation', 'Name', 'Income Size Flag'], axis=1)\n",
    "# Address: 'Address', 'CoreCaseGraphID', 'ExtendedCaseGraphID', 'testingFlag'\n",
    "v_address = v_sample[v_sample.Label == 'Address']\n",
    "v_address = v_address.drop(['Label', 'Revenue Size Flag', 'Account ID String', 'Person or Organisation', 'Name', 'Income Size Flag'], axis=1)\n",
    "# Customer: 'Person or Organisation', 'Name', 'Income Size Flag', 'CoreCaseGraphID', 'ExtendedCaseGraphID', 'testingFlag'\n",
    "v_customer = v_sample[v_sample.Label == 'Customer']\n",
    "v_customer = v_customer.drop(['Label', 'Revenue Size Flag', 'Account ID String', 'Address'], axis=1)\n",
    "# Derived Entity: 'Person or Organisation', 'Name', 'CoreCaseGraphID', 'ExtendedCaseGraphID', 'testingFlag'\n",
    "v_der_ent = v_sample[v_sample.Label == 'Derived Entity']\n",
    "v_der_ent = v_der_ent.drop(['Label', 'Revenue Size Flag', 'Account ID String', 'Address', 'Income Size Flag'], axis=1)\n",
    "# External Entity: 'Person or Organisation', 'Name', 'CoreCaseGraphID', 'ExtendedCaseGraphID', 'testingFlag'\n",
    "v_ext_ent = v_sample[v_sample.Label == 'Derived Entity']\n",
    "v_ext_ent = v_ext_ent.drop(['Label', 'Revenue Size Flag', 'Account ID String', 'Address', 'Income Size Flag'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes for each kynd of node (['Account', 'Address', 'Customer', 'Derived Entity', 'External Entity'])\n",
    "#In a smart way:\n",
    "v_sets = defaultdict()\n",
    "for v_type in list(pd.Categorical(v_sample.Label).categories):\n",
    "    v_sets[v_type] = v_sample[v_sample.Label == v_type]\n",
    "    v_sets[v_type] = v_sets[v_type].drop(['Label', 'testingFlag']+list(v_sets[v_type].columns[v_sets[v_type].isnull().all()]), axis=1)\n",
    "\n",
    "#create dataframes for each kynd of edge (['has account', 'has address', 'is similar', 'money transfer'])\n",
    "#In a smart way:\n",
    "e_sets = defaultdict()\n",
    "for e_type in list(pd.Categorical(e_sample.Label).categories):\n",
    "    e_sets[e_type] = e_sample[e_sample.Label == e_type]\n",
    "    e_sets[e_type] = e_sets[e_type].drop(['Label']+list(e_sets[e_type].columns[e_sets[e_type].isnull().all()]), axis=1)\n",
    "    e_sets[e_type] = e_sets[e_type].rename(columns={'from_id':'source', 'to_id':'target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert non numerical data in numerical data\n",
    "#1. \"logical\" conversion\n",
    "#Revenue Size Flag: low, mid_low, medium, mid_high, high -> 1,2,3,4,5\n",
    "conversion = {'low':1, 'mid_low':2, 'medium':3, 'mid_high':4, 'high':5}\n",
    "for i in v_sets:\n",
    "    if 'Revenue Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Revenue Size Flag']=v_sets[i]['Revenue Size Flag'].map(conversion)\n",
    "#Income Size Flag: low, medium, high -> 1,2,3\n",
    "conversion = {'low':1, 'medium':2, 'high':3}\n",
    "for i in v_sets:\n",
    "    if 'Income Size Flag' in list(v_sets[i].columns):\n",
    "        v_sets[i]['Income Size Flag']=v_sets[i]['Income Size Flag'].map(conversion)\n",
    "#Similarity Strength: weak, medium, strong -> 1,2,3\n",
    "conversion = {'weak':1, 'medium':2, 'strong':3}\n",
    "for i in e_sets:\n",
    "    if 'Similarity Strength' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Similarity Strength']=e_sets[i]['Similarity Strength'].map(conversion)\n",
    "#Amount Flag: small, medium, large -> 10,100,1000 (just to change the logic, the final choice is up to you) -> treated as weights\n",
    "conversion = {'small':10, 'medium':100, 'large':1000}\n",
    "for i in e_sets:\n",
    "    if 'Amount Flag' in list(e_sets[i].columns):\n",
    "        e_sets[i]['Amount Flag']=e_sets[i]['Amount Flag'].map(conversion)\n",
    "        e_sets[i] = e_sets[i].rename(columns={'Amount Flag':'weight'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. one-hot encoding\n",
    "#Person or Organisation: create 2 bool columns, one for Person, one for Organisation (could have just created a single boolean column: 0->Person, 1->Organization)\n",
    "for i in v_sets:\n",
    "    if 'Person or Organisation' in list(v_sets[i].columns):\n",
    "        v_sets[i] = pd.get_dummies(v_sets[i], columns=['Person or Organisation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. more complex transformations (i.e. from strings to numbers) -> the limit is your imagination!\n",
    "#Vertices: Account ID String, Address, Name\n",
    "#Fast solution: dropping non numerical attributes, but we're loosing lot of information\n",
    "for i in v_sets:\n",
    "    if 'Account ID String' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Account ID String', axis=1)\n",
    "    if 'Address' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Address', axis=1)\n",
    "    if 'Name' in list(v_sets[i].columns):\n",
    "        v_sets[i] = v_sets[i].drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Directed Graph in stellargraph \n",
    "G = sg.StellarDiGraph(v_sets, e_sets)\n",
    "\n",
    "# Print info about the graph we just built\n",
    "print(G.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Graph embedding with NODE2VEC and WORD2VEC\n",
    "rw = sg.data.BiasedRandomWalk(G)\n",
    "walks = rw.run(\n",
    "    nodes=list(G.nodes()),  # root nodes\n",
    "    length=10,  # maximum length of a random walk\n",
    "    n=10,  # number of random walks per root node\n",
    "    p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "    q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    ")\n",
    "print(\"Number of random walks: {}\".format(len(walks)))\n",
    "\n",
    "#import Word2Vec model from gensim library\n",
    "from gensim.models import Word2Vec\n",
    "#convert int ID to str ID, according to gensim library\n",
    "str_walks = [[str(n) for n in walk] for walk in walks]\n",
    "#train Representation Learning\n",
    "model = Word2Vec(str_walks, vector_size=128, window=5, min_count=0, sg=1, workers=8, epochs=5)\n",
    "# The embedding vectors can be retrieved from model.wv using the node ID.\n",
    "#model.wv[\"19231\"].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve node embeddings \n",
    "node_ids = model.wv.index_to_key  # list of node IDs\n",
    "node_embeddings = (model.wv.vectors)  # numpy.ndarray of size number of nodes times embeddings dimensionality\n",
    "\n",
    "# Retrieve corresponding targets\n",
    "# from training csv\n",
    "#core_trainings = core_training_sample.loc[[int(node_id) for node_id in node_ids if int(node_id) in list(core_training_sample.index)]].CaseID\n",
    "#ext_trainings = ext_training_sample.loc[[int(node_id) for node_id in node_ids if int(node_id) in list(ext_training_sample.index)]].CaseID\n",
    "# from vertices' data\n",
    "core_trainings = v_sample.loc[[int(node_id) for node_id in node_ids]].CoreCaseGraphID\n",
    "ext_trainings = v_sample.loc[[int(node_id) for node_id in node_ids]].ExtendedCaseGraphID\n",
    "\n",
    "# Transform the embeddings to 2d space for visualization\n",
    "transform = TSNE #PCA \n",
    "trans = transform(n_components=2)\n",
    "node_embeddings_2d = trans.fit_transform(node_embeddings)\n",
    "\n",
    "# draw the embedding points, coloring them by the target label (CaseID)\n",
    "alpha = 0.7\n",
    "label_map = {l: i for i, l in enumerate(np.unique(ext_trainings), start=10) if pd.notna(l)}\n",
    "node_colours = [label_map[target] if pd.notna(target) else 0 for target in ext_trainings]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.axes().set(aspect=\"equal\")\n",
    "plt.scatter(\n",
    "    node_embeddings_2d[:, 0],\n",
    "    node_embeddings_2d[:, 1],\n",
    "    c=node_colours,\n",
    "    cmap=\"jet\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "plt.title(\"{} visualization of node embeddings w.r.t. Extended Case ID\".format(transform.__name__))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Node classification with GraphSAGE, GCN, GAT\n",
    "\n",
    "# Split in train(70%), test(15%) and validation set (15%) \n",
    "train_ID, test_ID = model_selection.train_test_split(\n",
    "    ext_trainings, train_size=0.7, test_size=None, #stratify=ext_trainings\n",
    ")\n",
    "val_ID, test_ID = model_selection.train_test_split(\n",
    "    test_ID, train_size=0.5, test_size=None, #stratify=test_ID\n",
    ")\n",
    "print(len(train_ID.index), len(val_ID.index), len(test_ID.index))\n",
    "\n",
    "# Convert targets labels in one-hot encoded features (optional, for categorical targets)\n",
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "train_targets = target_encoding.fit_transform(train_ID)\n",
    "val_targets = target_encoding.transform(val_ID)\n",
    "test_targets = target_encoding.transform(test_ID)\n",
    "\n",
    "model_type = \"graphsage\" # gcn, gat\n",
    "use_bagging = (True)\n",
    "\n",
    "if model_type == \"graphsage\":\n",
    "    # For GraphSAGE model\n",
    "    batch_size = 50\n",
    "    num_samples = [10, 10]\n",
    "    n_estimators = 5  # The number of estimators in the ensemble\n",
    "    n_predictions = 10  # The number of predictions per estimator per query point\n",
    "    epochs = 50  # The number of training epochs\n",
    "elif model_type == \"gcn\":\n",
    "    # For GCN model\n",
    "    n_estimators = 5  # The number of estimators in the ensemble\n",
    "    n_predictions = 10  # The number of predictions per estimator per query point\n",
    "    epochs = 50  # The number of training epochs\n",
    "elif model_type == \"gat\":\n",
    "    # For GAT model\n",
    "    layer_sizes = [8, train_targets.shape[1]]\n",
    "    attention_heads = 8\n",
    "    n_estimators = 5  # The number of estimators in the ensemble\n",
    "    n_predictions = 10  # The number of predictions per estimator per query point\n",
    "    epochs = 200  # The number of training epochs\n",
    "\n",
    "if model_type == \"graphsage\":\n",
    "    generator = GraphSAGENodeGenerator(G, batch_size, num_samples)\n",
    "    train_gen = generator.flow(train_ID.index, train_targets, shuffle=True)\n",
    "elif model_type == \"gcn\":\n",
    "    generator = FullBatchNodeGenerator(G, method=\"gcn\")\n",
    "    train_gen = generator.flow(train_ID.index, train_targets)  # does not support shuffle\n",
    "elif model_type == \"gat\":\n",
    "    generator = FullBatchNodeGenerator(G, method=\"gat\")\n",
    "    train_gen = generator.flow(train_ID.index, train_targets)  # does not support shuffle \n",
    "\n",
    "if model_type == \"graphsage\":\n",
    "    base_model = GraphSAGE(\n",
    "        layer_sizes=[16, 16],\n",
    "        generator=generator,\n",
    "        bias=True,\n",
    "        dropout=0.5,\n",
    "        normalize=\"l2\"\n",
    "    )\n",
    "    x_inp, x_out = base_model.in_out_tensors()\n",
    "    predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)\n",
    "elif model_type == \"gcn\":\n",
    "    base_model = GCN(\n",
    "        layer_sizes=[32, train_targets.shape[1]],\n",
    "        generator=generator,\n",
    "        bias=True,\n",
    "        dropout=0.5,\n",
    "        activations=[\"relu\", \"softmax\"],\n",
    "    )\n",
    "    x_inp, predictions = base_model.in_out_tensors()\n",
    "elif model_type == \"gat\":\n",
    "    base_model = GAT(\n",
    "        layer_sizes=layer_sizes,\n",
    "        attn_heads=attention_heads,\n",
    "        generator=generator,\n",
    "        bias=True,\n",
    "        in_dropout=0.5,\n",
    "        attn_dropout=0.5,\n",
    "        activations=[\"relu\", \"softmax\"],\n",
    "    )\n",
    "    x_inp, predictions = base_model.in_out_tensors()\n",
    "\n",
    "model = Model(inputs=x_inp, outputs=predictions)\n",
    "\n",
    "if use_bagging:\n",
    "    model = BaggingEnsemble(model, n_estimators=n_estimators, n_predictions=n_predictions)\n",
    "else:\n",
    "    model = Ensemble(model, n_estimators=n_estimators, n_predictions=n_predictions)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.005),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "val_gen = generator.flow(val_ID.index, val_targets)\n",
    "test_gen = generator.flow(test_ID.index, test_targets)\n",
    "\n",
    "if use_bagging:\n",
    "    # When using bootstrap samples to train each model in the ensemble, we must specify\n",
    "    # the IDs of the training nodes (train_data) and their corresponding target values\n",
    "    # (train_targets)\n",
    "    history = model.fit(\n",
    "        generator,\n",
    "        train_data=train_gen.index,\n",
    "        train_targets=train_targets,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        verbose=0,\n",
    "        shuffle=False,\n",
    "        bag_size=None,\n",
    "        use_early_stopping=True,  # Enable early stopping\n",
    "        early_stopping_monitor=\"val_acc\",\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_gen,\n",
    "        verbose=0,\n",
    "        shuffle=False,\n",
    "        use_early_stopping=True,  # Enable early stopping\n",
    "        early_stopping_monitor=\"val_acc\",\n",
    "    )\n",
    "\n",
    "sg.utils.plot_history(history)\n",
    "\n",
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe92909fe2aa863366b8c35cb109d5b987c372b7c57e10826f4bc189b39699a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
